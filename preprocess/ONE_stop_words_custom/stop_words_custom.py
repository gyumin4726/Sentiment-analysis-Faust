import re
from collections import Counter
import string

# ì œê±°í•  ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ê³ ì „ ì˜ì–´ + ì¼ë°˜ ë¶ˆìš©ì–´)
custom_stopwords = [
    "thou", "thee", "thy", "â€™tis", "art", "ye", "hath", "dost", "doth", "shalt", "wherefore",
    "one", "it", "us", "must", "may", "let",
    "shall", "would",
    "english", "illustration",
    'me', 'yet', 'then', 'now', 'still', 'hast', 'mr', 'thus',
    'original', 'form', 'translation', 'even', 'many', 'be', 'well', 'upon', 'here', 'â€™'
]


def clean_text(text):
    """
    1. [ ] ë° ( ) ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ì œê±°
    2. ë¬¸ì¥ ë¶€í˜¸ ì œê±°
    3. ë¶ˆìš©ì–´ ì œê±°
    """
    text = re.sub(r"\[.*?\]|\(.*?\)", "", text)  # [ ]ì™€ ( ) ì•ˆì˜ ë‚´ìš© ì œê±°
    text = text.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜
    text = text.translate(str.maketrans("", "", string.punctuation))  # ë¬¸ì¥ ë¶€í˜¸ ì œê±°
    words = text.split()  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
    filtered_words = [word for word in words if word not in custom_stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    return " ".join(filtered_words)  # ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜

def get_filtered_word_frequencies(text_lines):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  [ ], ( ) ì•ˆì˜ ë‚´ìš©ì„ ì‚­ì œí•œ í›„ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
    """
    all_words = []
    
    for line in text_lines:
        cleaned_line = clean_text(line)
        words = cleaned_line.split()  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        all_words.extend(words)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    
    word_counts = Counter(all_words)  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
    return word_counts

# ğŸ“Œ ìˆ˜ì •ëœ íŒŒì¼ ê²½ë¡œ (ë°±ìŠ¬ë˜ì‹œ ë¬¸ì œ í•´ê²°: r"" ë˜ëŠ” os.path ì‚¬ìš©)
file_path = r"C:\Users\ë°•ê·œë¯¼\OneDrive - KookminUNIV\ë°”íƒ• í™”ë©´\ë¹…ë°ì´í„° ìµœì‹ ê¸°ìˆ \Sentiment-analysis-Faust\data\Faust [part 1]. Translated Into English in the Original Metres by Goethe.txt"

with open(file_path, "r", encoding="utf-8") as f:
    faust_text = f.readlines()

cleaned_no_stopwords = [clean_text(line) for line in faust_text]

# ë¶ˆìš©ì–´ ì œê±° + ëŒ€ì‚¬ ì •ì œ í›„ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
filtered_faust_word_counts = get_filtered_word_frequencies(faust_text)

# ìƒìœ„ 20ê°œ ë‹¨ì–´ ì¶œë ¥
print("ğŸ“Œ íŒŒìš°ìŠ¤íŠ¸ ëŒ€ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ 20ê°œ (ë¶ˆìš©ì–´ ì œê±° + ì •ì œ í›„):")
print(filtered_faust_word_counts.most_common(20))

# ê²°ê³¼ ì €ì¥
faust_no_stopwords_path = "faust_cleaned_no_stopwords.txt"
mephi_no_stopwords_path = "mephi_cleaned_no_stopwords.txt"

with open(faust_no_stopwords_path, "w", encoding="utf-8") as f:
    f.write("\n".join(cleaned_no_stopwords))


print("âœ… ë¶ˆìš©ì–´ ì œê±° ì™„ë£Œ! ì •ë¦¬ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")