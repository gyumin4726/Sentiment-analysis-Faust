from collections import Counter
import string
import os

def get_word_frequencies(text_lines):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    """
    all_words = []
    
    for line in text_lines:
        line = line.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜
        line = line.translate(str.maketrans("", "", string.punctuation))  # ë¬¸ì¥ ë¶€í˜¸ ì œê±°
        words = line.split()  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        all_words.extend(words)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    
    word_counts = Counter(all_words)  # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
    return word_counts

# ğŸ“Œ ìˆ˜ì •ëœ íŒŒì¼ ê²½ë¡œ (ë°±ìŠ¬ë˜ì‹œ ë¬¸ì œ í•´ê²°: r"" ë˜ëŠ” os.path ì‚¬ìš©)
faust_file_path = r"C:\Users\ë°•ê·œë¯¼\OneDrive - KookminUNIV\ë°”íƒ• í™”ë©´\ë¹…ë°ì´í„° ìµœì‹ ê¸°ìˆ \Sentiment-analysis-Faust\preprocess\stop_words_default\faust_cleaned_no_stopwords.txt"
mephi_file_path = r"C:\Users\ë°•ê·œë¯¼\OneDrive - KookminUNIV\ë°”íƒ• í™”ë©´\ë¹…ë°ì´í„° ìµœì‹ ê¸°ìˆ \Sentiment-analysis-Faust\preprocess\stop_words_default\mephi_cleaned_no_stopwords.txt"

# íŒŒì¼ ì½ê¸°
with open(faust_file_path, "r", encoding="utf-8", errors="replace") as f:
    faust_text = f.readlines()

with open(mephi_file_path, "r", encoding="utf-8", errors="replace") as f:
    mephi_text = f.readlines()

# ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
faust_word_counts = get_word_frequencies(faust_text)
mephi_word_counts = get_word_frequencies(mephi_text)

# ìƒìœ„ 20ê°œ ë‹¨ì–´ ì¶œë ¥
print("ğŸ“Œ íŒŒìš°ìŠ¤íŠ¸ ëŒ€ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ 20ê°œ:")
for word, count in faust_word_counts.most_common(20):
    print(f"{word}: {count}")

print("\nğŸ“Œ ë©”í”¼ìŠ¤í† í ë ˆìŠ¤ ëŒ€ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ 20ê°œ:")
for word, count in mephi_word_counts.most_common(20):
    print(f"{word}: {count}") 
